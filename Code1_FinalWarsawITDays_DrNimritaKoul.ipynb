{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building and Evaluating Multimodal AI\n",
        "\n",
        "## Author: Dr. Nimrita Koul\n",
        "## www.linkedin.com/in/nimritakoul"
      ],
      "metadata": {
        "id": "_Kq-7fmf0mfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task1 : Retrieving Embeddings and Verifying the Similarity Between Them\n",
        "\n",
        "1. Use the transformers library from HuggingFace to load CLIP Input processor and the corresponding model\n",
        "2. Load an image and some text descriptions\n",
        "3. Use the CLIPProcessor to learn joint embeddings of the image and the text statements.\n",
        "4. Extract the image and the text embeddings learned by the CLIPProcessor.\n",
        "5. Calculate cosine similarity between the image embedding and each one of the text embeddings.\n",
        "6. Verify that the most relevant text caption has the highest similarity score with the image embedding."
      ],
      "metadata": {
        "id": "T-R4_lDsjUJg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cpkTJgL8DM4"
      },
      "outputs": [],
      "source": [
        "#Required Imports\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "from IPython.display import  display\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_clip_similarity(model, processor, image, texts):\n",
        "    \"\"\"\n",
        "    Compute CLIP similarity scores between an image and multiple text descriptions.\n",
        "\n",
        "    Args:\n",
        "        model: Loaded CLIP model\n",
        "        processor: Loaded CLIP processor\n",
        "        image: PIL Image object\n",
        "        texts: List of text descriptions\n",
        "\n",
        "    Returns:\n",
        "        tuple: (similarities, probs, best_text, best_similarity)\n",
        "    \"\"\"\n",
        "    # Preprocess inputs using CLIP processor\n",
        "    inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Extract embeddings from CLIP Model\n",
        "    with torch.no_grad():\n",
        "        image_features = model.get_image_features(inputs[\"pixel_values\"])  # Image embedding\n",
        "        text_features = model.get_text_features(inputs[\"input_ids\"], inputs[\"attention_mask\"])  # Text embeddings\n",
        "\n",
        "    # Normalize embeddings for cosine similarity\n",
        "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarities = (image_features @ text_features.T).squeeze(0)\n",
        "    probs = similarities.softmax(dim=0)\n",
        "\n",
        "    # Display results\n",
        "    for text, sim, prob in zip(texts, similarities, probs):\n",
        "        print(f\"Text: '{text}' | Similarity: {sim.item():.4f} | Probability: {prob.item():.4f}\")\n",
        "\n",
        "    # Find best match\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_text = texts[best_idx]\n",
        "    best_similarity = similarities[best_idx]\n",
        "    print(f\"\\nBest aligned text: '{best_text}' | Similarity: {best_similarity:.4f}\")\n",
        "\n",
        "    return image_features, text_features, best_idx, similarities\n"
      ],
      "metadata": {
        "id": "TFEGaw03UNFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You need to a Hugging Face account and a valid access token to access models from Hugging Face Hub.\n",
        "\n",
        "\n",
        "Creat you free HF account here:\n",
        "https://huggingface.co/\n",
        "\n",
        "Creat your HF access token here:\n",
        "https://huggingface.co/settings/account"
      ],
      "metadata": {
        "id": "bAqwgfT1VH7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with your existing code:\n",
        "# Load CLIP model and processor\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "id": "mwlXRaTgU5vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let us try with an image of Indian Food"
      ],
      "metadata": {
        "id": "9O2R2W-_ka8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample image\n",
        "image_food = Image.open(\"Indianfood.jpg\").convert(\"RGB\")\n",
        "#display(image_food)\n",
        "# Text descriptions\n",
        "texts = [\n",
        "    \"A historic city square in Warsaw\",\n",
        "    \"A beach with palm trees\",\n",
        "    \"Delicious Indian Food\"\n",
        "]\n",
        "# Call the function\n",
        "image_features, text_features, best_idx, similarities = compute_clip_similarity(model, processor, image_food, texts)\n"
      ],
      "metadata": {
        "id": "2031nCvQVAXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Now, let us try with  an image of Warsaw City Square"
      ],
      "metadata": {
        "id": "vvj3Q4LvWNUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_Warsaw = Image.open(\"Warsaw.jpg\").convert(\"RGB\")\n",
        "#display(image_Warsaw)\n",
        "\n",
        "\n",
        "# Text descriptions\n",
        "texts_Warsaw = [\n",
        "    \"A historic city square in Warsaw\",\n",
        "    \"A beach with palm trees\",\n",
        "    \"Delicious Indian Food\"\n",
        "]\n",
        "\n",
        "# Call the function\n",
        "image_features, text_features, best_idx, similarities = compute_clip_similarity(model, processor, image_Warsaw, texts)\n"
      ],
      "metadata": {
        "id": "P0KBUJvP-1QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2:  Investigate the Effect of Fusion Strategies on Model Response\n",
        "\n",
        "In this task, we will frame a binary classification task:\n",
        "\n",
        "given an image and its true  caption (positive sample) we will check the confidence with which a classifier correctly classifies this true pair and the influence of fusion strategy on this confidence.\n",
        "\n",
        "We will check the influence of these 3 fusion techniques:\n",
        "\n",
        "- Early Fusion\n",
        "- Late Fusion\n",
        "- Cross-Modal Attention\n",
        "\n",
        "\n",
        "## First let us extract the embeddings of the image and the corresponding caption as in previous cells:"
      ],
      "metadata": {
        "id": "Y7aBJkk7jqig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## This cell contains code just like the previous cell for similarity calculation.\n",
        "# Prerequisites: pip install transformers torch Pillow requests numpy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "# Load CLIP model and processor\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
      ],
      "metadata": {
        "id": "NOpe06XT--m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute clip similarity on image of Warsaw and All captions"
      ],
      "metadata": {
        "id": "HTstDpJVfWYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to compute clip similarity on image of Warsaw and All captions\n",
        "image_features, text_features, best_idx, similarities = compute_clip_similarity(model, processor, image_Warsaw, texts_Warsaw)"
      ],
      "metadata": {
        "id": "9x5IEsVXXjcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Fusion Techniques (using first text as positive example)\n",
        "#Extract image embedding\n",
        "image_emb = image_features  # [1, 512]\n",
        "\n",
        "# Extract the text embedding of its matching caption (the first one in our dataset texts_Warsaw)\n",
        "text_emb = text_features[0:1]  # [1, 512], \"A historic city square in Warsaw\"\n",
        "\n",
        "# This is a positive sample - (image and caption are a match, so the label is 1.0)\n",
        "label = torch.tensor([1.0])  # Ground truth: match\n",
        "\n",
        "#Let us implement three fusion schemes one by one\n",
        "\n",
        "# 1. Early Fusion\n",
        "class EarlyFusionModel(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) #One Hidden Layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1) # The output layer with one node\n",
        "\n",
        "    def forward(self, image_emb, text_emb):\n",
        "        #Fuse embeddings before passing through the classifier network\n",
        "        fused = torch.cat((image_emb, text_emb), dim=1)  # [1, 1024]\n",
        "        x = F.relu(self.fc1(fused))#pass the fused embeddings through hidden layer\n",
        "        #The output is the probability that image and text embeddings are a positive pair.\n",
        "        return torch.sigmoid(self.fc2(x))#pass activations through output layer and sigmoid\n",
        "\n",
        "# 2. Late Fusion\n",
        "class LateFusionModel(nn.Module):\n",
        "    def __init__(self, input_dim=512, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.image_branch = nn.Linear(input_dim, hidden_dim) # Hidden layer for image embeddings\n",
        "        self.text_branch = nn.Linear(input_dim, hidden_dim)# Hidden layer for text embeddings\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1) #Output layer with one node\n",
        "\n",
        "    def forward(self, image_emb, text_emb):\n",
        "        #separately process the image and text embeddings\n",
        "        img_out = F.relu(self.image_branch(image_emb)) #Pass image embeddings through separate hidden layer\n",
        "        txt_out = F.relu(self.text_branch(text_emb))#Pass text embeddings through separate hidden layer\n",
        "        #Fuse the outputs for image and text before passing to the final classification layer\n",
        "        fused = torch.cat((img_out, txt_out), dim=1)  # [1, 256]\n",
        "        #The output is the probability that image and text embeddings are a positive pair.\n",
        "        return torch.sigmoid(self.fc(fused))\n",
        "\n",
        "# 3. Cross-Modal Attention\n",
        "class CrossModalAttentionModel(nn.Module):\n",
        "    def __init__(self, embed_dim=512, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads=4)#MHA layer\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim) # one hidden layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1) #one output layer with one node\n",
        "\n",
        "    def forward(self, image_emb, text_emb):\n",
        "        image_emb = image_emb.unsqueeze(0)  # [1, 1, 512]\n",
        "        text_emb = text_emb.unsqueeze(0)    # [1, 1, 512]\n",
        "        attn_out, _ = self.attn(text_emb, image_emb, image_emb) #compute attention score of image and text embeddings\n",
        "        x = F.relu(self.fc1(attn_out.squeeze(0))) #pass the attention scores through hidden layer\n",
        "        #The output is the probability that image and text embeddings are a positive pair.\n",
        "        return torch.sigmoid(self.fc2(x))"
      ],
      "metadata": {
        "id": "r-_e5F9ykHGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate and run fusion strategies\n",
        "early_model = EarlyFusionModel()\n",
        "late_model  = LateFusionModel()\n",
        "attn_model  = CrossModalAttentionModel()\n",
        "\n",
        "print(\"\\nFusion Results (Text: 'A historic city square in Warsaw'):\")\n",
        "early_output = early_model(image_emb, text_emb)\n",
        "print(f\"Early Fusion Prediction: {early_output.item():.4f} | Label: {label.item()}\")\n",
        "\n",
        "late_output = late_model(image_emb, text_emb)\n",
        "print(f\"Late Fusion Prediction: {late_output.item():.4f} | Label: {label.item()}\")\n",
        "\n",
        "attn_output = attn_model(image_emb, text_emb)\n",
        "print(f\"Cross-Modal Attention Prediction: {attn_output.item():.4f} | Label: {label.item()}\")\n"
      ],
      "metadata": {
        "id": "_xdbAddSY7rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Evaluating MultiModal RAG\n",
        "\n",
        "## Steps:\n",
        "\n",
        "1. We wil give a text query to a multimodal model and ask it to retrieve a relevant image and its caption from a small dataset of images and captions.\n",
        "\n",
        "2. Then we will ask the model to generate a detailed description of the retrieved image.\n",
        "\n",
        "3. Finally, we will evaluate the generated description against the caption in our dataset or a reference caption.\n",
        "\n",
        "## Components in this RAG Task:\n",
        "- Retrieval: Use CLIP model retrieve image and its caption based on query (context).\n",
        "- Generation: Use GPT-2 to generate text description using retrieved context.\n",
        "- Evaluation: Use\n",
        "1. BLEU score to compare generated description to a reference description.\n",
        "2. LLM as a judge, we use GPT2-medium as a judge to evaluate the generated description."
      ],
      "metadata": {
        "id": "LyjSKbBfNwtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In below cell,\n",
        "- we load CLIP models for image+caption retrieval,\n",
        "- load gpt2 model for generation of text description for retrieved image.\n",
        "- define our image+text dataset"
      ],
      "metadata": {
        "id": "G8fU4NuZv4uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prerequisites: pip install transformers torch Pillow requests numpy nltk\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel, GPT2Tokenizer, GPT2LMHeadModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "import io\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "TIZlZki2ZKdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models for multimodal image+caption retrieval\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "#gpt2 for generation of description of the retrieved image\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
      ],
      "metadata": {
        "id": "eFC13ap4ZLsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us create a small dataset (image + text pairs)\n",
        "knowledge_base = [\n",
        "    {\n",
        "        \"image_url\": \"Warsaw.jpg\",\n",
        "        \"caption\": \"A historic city square in Warsaw with colorful buildings.\"\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"image_url\": \"Beach.jpg\",\n",
        "        \"caption\": \"A beautiful beach with crystal clear waters.\"\n",
        "    },\n",
        "    {\n",
        "        \"image_url\": \"Palm.jpg\",\n",
        "        \"caption\": \"Luscious palm trees under a sunny sky.\"\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"image_url\": \"Indianfood.jpg\",\n",
        "        \"caption\": \"Delicious Indian Food.\"\n",
        "    }\n",
        "\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "fhp07k7ZN2iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In below cell,\n",
        "- we are adding all images to an images[] list, and all captions to a captions[] list\n",
        "- and displaying the images"
      ],
      "metadata": {
        "id": "NnMVNaEkwLkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess knowledge base images\n",
        "images = []\n",
        "captions = []\n",
        "for item in knowledge_base:\n",
        "    try:\n",
        "        img = Image.open(item['image_url']).convert(\"RGB\")\n",
        "        display(img)\n",
        "        images.append(img)\n",
        "        captions.append(item[\"caption\"])\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {item['image_url']}: {e}\")\n"
      ],
      "metadata": {
        "id": "vKp2jgh_czMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In the below cell:\n",
        "\n",
        "1. we define a query describing an image\n",
        "2. then we use CLIP model to retrieve the image and its corresponding caption from our dataset that match the query query."
      ],
      "metadata": {
        "id": "OSEUDKOAwaw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query provides the context for retrieval, you can use your documents for this too.\n",
        "query = \"Describe a historic square with colourful, tall buildings\"\n",
        "\n",
        "# Step 1: Retrieval with CLIP\n",
        "inputs = clip_processor(text=[query] + captions, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "#get embeddings\n",
        "with torch.no_grad():\n",
        "    image_features = clip_model.get_image_features(inputs[\"pixel_values\"])  # [2, 512]\n",
        "    text_features = clip_model.get_text_features(inputs[\"input_ids\"], inputs[\"attention_mask\"])  # [3, 512]\n",
        "\n",
        "\n",
        "# Normalize and compute similarities\n",
        "image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "#compute similarity score between text embeddings and image embedding\n",
        "similarities = (text_features[0:1] @ image_features.T).squeeze(0)  # Query vs. images\n",
        "\n",
        "#identify the index of best image that matches the query and captions\n",
        "best_idx = similarities.argmax().item()\n"
      ],
      "metadata": {
        "id": "xYCPbJZFdUfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieved context\n",
        "retrieved_image = images[best_idx] #image that best matches the query\n",
        "retrieved_caption = captions[best_idx] #caption that best matches the retrieved image\n",
        "display(retrieved_image)\n",
        "print(f\"Retrieved Caption: '{retrieved_caption}'\")\n"
      ],
      "metadata": {
        "id": "G1tptUMsckAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, we use gpt2 text generator model to generate a description based on query defined above and the caption retrieved from our dataset by CLIP(text to text generation task)."
      ],
      "metadata": {
        "id": "v0610lqExeQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Description Generation with GPT-2\n",
        "# Construct your prompt with query and retrieved caption\n",
        "prompt = f\"Query: {query}\\nContext: {retrieved_caption}\\nDescription: \"\n",
        "\n",
        "# instantiate gpt2_tokenizer\n",
        "inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "#call the generate() API on inputs\n",
        "outputs = gpt2_model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=50,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    do_sample=True,\n",
        "    pad_token_id = gpt2_tokenizer.eos_token_id,\n",
        "    attention_mask = inputs[\"attention_mask\"],\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "# decode the outputs\n",
        "generated_text = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\\nGenerated Description:\\n{generated_text}\")\n"
      ],
      "metadata": {
        "id": "m8nwQfwBdNZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, let us evaluate the description generated by gpt2 model against a Reference Text Description using BLUE Score"
      ],
      "metadata": {
        "id": "7O7JcSr_aFXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Evaluation with BLEU\n",
        "# This is reference text description to evaluate against\n",
        "reference = \"A historic city square in Warsaw with colorful buildings and cobblestone streets.\"\n",
        "\n",
        "#tokenize reference description\n",
        "ref_tokens = reference.split()\n",
        "#tokenize generated description (generated_text variable from previous cell)\n",
        "gen_tokens = generated_text.split(\"Description: \")[1].split()\n",
        "\n",
        "# Print for debugging\n",
        "print(f\"Reference Tokens: {ref_tokens}\")\n",
        "print(f\"Generated Tokens: {gen_tokens}\")\n"
      ],
      "metadata": {
        "id": "fNB3ieHfc-sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  BLEU (Bilingual Evaluation Understudy) score counts the number of overlapping tokens (n-grams) in two or more pieces of text. Scores are normalized to be between 0 and 1. Higher scores indicate better alignment between two pieces of text.\n",
        "\n",
        "### We are counting Unigram overlap between the generated and reference description here:"
      ],
      "metadata": {
        "id": "DaRVFOvZ6QEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate BLEU with smoothing\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "smoothie = SmoothingFunction().method1\n",
        "bleu_score = sentence_bleu([ref_tokens], gen_tokens, smoothing_function=smoothie)\n",
        "print(f\"\\nBLEU Score with Smoothing: {bleu_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "KVb3ZqtQe3Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next, we will evaluate the Generated Description using another LLM as a Judge\n",
        "\n",
        "Steps:\n",
        "1. Use gpt2-medium model as a judge\n",
        "2. Design a prompt for the LLM judge\n",
        "3. Invoke the judge LLM to evaluate the reference nad generated description"
      ],
      "metadata": {
        "id": "9l-hOTdFulQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Instantiate gpt2-medium model and its tokenizer for text-generation\n",
        "evaluator = pipeline(\"text-generation\", model=\"gpt2-medium\", tokenizer=\"gpt2-medium\")\n",
        "\n",
        "# Configure the tokenizer\n",
        "evaluator.tokenizer.pad_token = evaluator.tokenizer.eos_token\n",
        "evaluator.tokenizer.pad_token_id = evaluator.tokenizer.eos_token_id\n",
        "\n",
        "## Define the function that uses an LLM judge to evaluate two texts\n",
        "def llm_judge_with_model(reference, generated_text):\n",
        "    ## Design the prompt for LLM Judge\n",
        "    prompt = (\n",
        "        f\"You are an evaluator. Compare the following texts and assign a numeric score from 0 to 10 based on relevance, coherence, and factual accuracy.\\n\"\n",
        "        f\"Provide your response in this format:\\n\"\n",
        "        f\"Score: [number]/10\\n\"\n",
        "        f\"Explanation: [Your reasoning]\\n\\n\"\n",
        "        f\"Example 1:\\n\"\n",
        "        f\"Reference: 'The sky is blue and vast.'\\n\"\n",
        "        f\"Generated: 'The sky is blue and wide.'\\n\"\n",
        "        f\"Score: 8/10\\n\"\n",
        "        f\"Explanation: The generated text is relevant and coherent, with minor variation in word choice.\\n\\n\"\n",
        "        f\"Example 2:\\n\"\n",
        "        f\"Reference: 'A tall mountain in the Alps.'\\n\"\n",
        "        f\"Generated: 'A small hill in France.'\\n\"\n",
        "        f\"Score: 3/10\\n\"\n",
        "        f\"Explanation: The generated text is less relevant and factually inaccurate.\\n\\n\"\n",
        "        f\"Now evaluate the following pair of reference and generated text:\\n\"\n",
        "        f\"Reference: '{reference}'\\n\"\n",
        "        f\"Generated: '{generated_text}'\\n\"\n",
        "    )\n",
        "\n",
        "    #Call the Judge\n",
        "    response = evaluator(prompt,max_new_tokens=50,num_return_sequences=1,pad_token_id=evaluator.tokenizer.pad_token_id)[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "    #response = evaluator(prompt, max_length=200, num_return_sequences=1, truncation=True)[0][\"generated_text\"]\n",
        "    print(\"\\n--- LLM Judgment (Real Model) ---\")\n",
        "\n",
        "    # regular expression code to extract numeric score from judge LLM's output\n",
        "    import re\n",
        "    score_match = re.search(r\"Score:\\s*(\\d+|X)/10\", response)\n",
        "    explanation_match = re.search(r\"Explanation:\\s*(.+)\", response)\n",
        "\n",
        "    if score_match:\n",
        "        score_str = score_match.group(1)\n",
        "        score = int(score_str) if score_str.isdigit() else 5\n",
        "    else:\n",
        "        score = 5\n",
        "    explanation = explanation_match.group(1) if explanation_match else \"Could not parse response properly.\"\n",
        "\n",
        "    #Print the score and the explanation from the response\n",
        "    print(f\"Score: {score}/10\")\n",
        "    print(f\"Explanation: {explanation}\")\n",
        "    return score\n",
        "\n",
        "\n",
        "# Specify a reference description and call the Judge LLM to evaluate\n",
        "reference = \"A historic city square in Warsaw with colorful buildings and cobblestone streets.\"\n",
        "\n",
        "#Call the function to evaluate two texts\n",
        "llm_score = llm_judge_with_model(reference, generated_text)\n"
      ],
      "metadata": {
        "id": "hVjFGC3_zM9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap: In this Notebook, we saw three multimodal tasks in action:\n",
        "1. Retrieving multimodal embeddings from CLIP model and verifying their similarity scores.\n",
        "2. Verifying the impact of fusion strategies on the performance of the classifier head of an AI model.\n",
        "3. Evaluating a multimodal RAG application using BLEU score and LLM as a judge."
      ],
      "metadata": {
        "id": "mVb_HQpFzCjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#References:\n",
        "\n",
        "[1].Multimodal Deep Learning, https://people.csail.mit.edu/khosla/papers/icml2011_ngiam.pdf\n",
        "\n",
        "[2].https://slds-lmu.github.io/seminar_multimodal_dl/c02-00-multimodal.html#c02-05-text-plus-img\n",
        "\n",
        "[3].https://cmu-multicomp-lab.github.io/mmml-course/fall2020/\n",
        "\n",
        "[4].https://huyenchip.com/2023/10/10/multimodal.html\n",
        "\n",
        "[5].https://github.com/huggingface/transformers/blob/main/examples/pytorch/contrastive-image-text/README.md\n",
        "\n",
        "[6]. https://sites.research.google/med-palm/\n"
      ],
      "metadata": {
        "id": "lFlmD9q405yV"
      }
    }
  ]
}